---
title: 【连续学习】灾难性遗忘
tags:
  - Paper Reading
categories:
  - - 机器学习
    - 连续学习

---


当神经网络用于学习任务序列时，后面任务的学习可能会降低前面任务学习模型的性能。这种问题被称为*灾难性遗忘*。为了保持模型的可持续学习能力，实现终生学习，解决灾难性遗忘尤为重要。

<!-- more-->

## 1. 灾难性遗忘 Catastrophic Forgetting

### 1.1 简介  

​		*灾难性遗忘*或*灾难性干扰*最早由McCloskey和Cohen[1989]认识到。他们发现，当对新的任务或类别进行训练时，神经网络往往可能会覆盖过去所学的权重，从而降低模型对过去任务的性能。如果不解决这个问题，单个神经网络将无法自己适应LL场景，因为在学习新的东西时，会*忘记*现有的信息/知识。这在Abraham和Robins[2005]中也被称为稳定性-可塑性困境。一方面，如果一个模型过于稳定，他将无法从未来的训练数据中消化新的信息。另一方面，一个模型如果因权重的大幅度变化而有具有极高的可塑性，那么它则更加容易忘记以前的学习表征。我们应该注意到，灾难性遗忘发生在传统的多层感知器以及DNN上。影子单层模型，如自组织特征图，也被证明有灾难性干扰[Richardson 和 Thomas, 2008]。

​	灾难性遗忘的一个具体例子是使用深度神经网络进行转移学习。在点行的转移学习环境中，源域有大量的标签数据，而目标域的标签数据很少，*fine-turnning*在DNN中被广泛使用[Dauphin等, 2012]，以使得源域的模型适应目标域。在*fine-turning*之前，源域的标签数据被用来预训练神经网络。然后给定目标域数据，对该神经网络的输出层进行再训练。基于反向传播的*fine-turning*则被应用于使源模型适应目标域，然而，这样的方法仍然收到灾难性遗忘的负面影响，因为对目标域的适应通常会破坏源域学习的权重，导致源域的推理效果不佳。

### 1.2 描述

Li和Hoiem[2016]对处理灾难性遗忘的传统方法进行了很好的概述。他们对典型方法中的三组参数进行了描述：

•     *θs*：所有任务共享的参数集合

•     *θo*： 为以前文物专门学习的参数集

•     *θn*：随机初始化新任务的任务规格参数

Li和Hoiem[2016]在图像分类的背景下给出了一个例子，其中*θs*由AlexNet架构中的五个卷积层和两个完全连接的层组成[Krizhevsky等, 2012]，*θo*是分类的输出层[Russakovsky等, 2015]及其相应的权重，*θ**n*是新任务的输出层，例如场景分类器。

​    传统由*θs*知识转移到*θn*的学习方法有三种：

•     **特征提取**（如Donahue等[2014]）：*θs*和*θo*都保持不变，而一些层的输出被用作新任务训练*θn*的特征

•     **微调**（如Dauphin等[2012]）：*θs*和*θn*被优化并针对新任务进行更新，而θo保持不变。为了防止*θn*的大转变，通常采用拉低学习率。此外，对于类似的目的，网络为每个新的任务可以*重复**fine-turned*，使*N*个网络对应*N*个任务。另一种变化是对*θs*的部分进行重新调优，例如，顶层。这可以被看作是特征提取和*fine-turning*的这种。

•     **联合训练**（如Caruana[1997]）：所有的参数*θ**s**，θo，θn*在所有任务中联合优化。这需要存储所有任务的所有训练数据。多任务学习（MTL）通常采用这种方法。

这些方法的优缺点总结在表4.1中。根据这些优缺点，Li和Hoiem[2016]提出了一种“Learning without Forgetting”的算法，明确处理这些方法的弱点（见4.3节）

表4.1：处理灾难性遗忘的传统方法摘要。改编自Li 和Hoiem[2016]

| 种类                 | 特征提取 | 微调 | 复制和微调 | 联合训练 |
| -------------------- | -------- | ---- | ---------- | -------- |
| 学习新任务效果       | 中等     | 好   | 好         | 好       |
| 旧任务效果           | 好       | 差   | 好         | 好       |
| 训练效率             | 快速     | 快速 | 快速       | 慢       |
| 测试效率             | 快速     | 快速 | 慢         | 快速     |
| 存储要求             | 中等     | 中等 | 高         | 高       |
| 是否需要以前任务数据 | 否       | 否   | 否         | 是       |





## 2. 神经网络中的持续学习

​    最近的一些研究提出了一些持续学习的方法来减少灾难性遗忘。本节主要为这些较新的发展做一个概述。Parisi等[2018a]中也给出了关于同一主题的全面调查。

### 2.1 监督学习

#### 2.1.1 对参数的调整

​    现有的大部份工作都集中在*监督学习*上[Parisi等，2018a]。受到*fine-turning*的启发，Rusu等[2016]提出了一种渐进式神经网络，它保留了一个预先训练好的模型池，并学习他们之间的横向链接。Kirkpatick等[2017]提出了一个名为弹性权重巩固（EWC）模型，量化权重对之前任务的重要性，并选择地调整权重的可塑性。Rebuffi 等[2017]通过保留一个最接近之前任务的示例集来解决LL问题。Alijundi等[2016]提出了一种专家网络来衡量处理灾难性遗忘的任务相关度。Rannen EpTriki等[2017]利用自动编码器的思想对“Learning without Forgetting”[Li和Hoiem,2016]中的方法进行了扩展。Shin等[2017]遵循生成对抗网络（GANs）框架[Goodfellow, 2016]，为之前的任务保留一组生成器，然后学习参数，以适应新任务的真实数据和之前任务的重返数据的混合集合。所有的这些工作将会在接下来的几节中详细介绍。

#### 2.1.2 重新归纳

Jung等[2016]没有像“Learing without Forgetting”（LwF）[Li 和 Hoiem, 2016]模型那样使用知识提炼，而是提出了一种少遗忘的学习，将优先的隐藏激活重新归纳。Rosenfeld和Tsotsos[2017]提出了控制器模块，以优化新任务上的损失域从前任务中学习的表征。他们发现的参数，同时相对于*fine-turnning*方法22%的参数，就可以达到令人满意的性能。Ans等[2004]设计了一种双网络的架构来生成伪项，用来自我刷新之前的任务。Jin和Sendho[2006]将灾难性以往问题建模为一个多目标学习问题，并提出了一个多目标的伪演练框架，以在optimization过程中交错使用基础模式和新模式。Nguyen等[2017]结合神经网络的在线变异推理（VI）和蒙特卡洛VI，提出了编译持续学习。在EWC[Kirkpatrick 等，2017]的激励下，Zenke等[2017]以在线方式测量突触巩固强度，并将其作为神经网络的正则化。Seff等[2017]提出结合GANs[Goodfellow，2016]和EWC[Kirkpatrick等,2017]的思想来解决持续生成模型。

#### 2.1.3 双记忆学习系统

​    除了上面提到的基于正则化的方法（例如LwF[Li 和 Hoiem, 2016]，EWC[Kirkpatrick等,2017]），基于双记忆的学习系统也被提出来用于LL。它们的灵感来自于互补学习系统（CLS）理论[Kumaran等，2016，McClelland等，1995]，其中及以巩固和减速域哺乳动物海马（短期记忆）和新皮质（长期记忆）的相互作用有关。Gepperth和Karaoguz[2016]提出使用修改后的自主之地图（SOM）作为长期记忆。作为补充，增加了一个短期记忆（STM）来存储新颖的例子。在睡眠阶段STM的全部内容被重放给系统。这个过程被称为内在重放或伪演练[Robins, 1995]。它用新的数据（如来自STM的数据集）和以前看到的类或分布的重放样本来训练网络中的所有节点，网络已经在这些数据上进行了训练。重新播放的样本可以防止网络遗忘，Kemker和Kanan[2018]提出了一个类似的双记忆系统，称为FearNet。它使用一个海马网络用于STM，一个内侧前额叶皮层（mPFC）网络用于长期记忆，以及第三个神经网络来检测使用哪种记忆进行预测，最近在这个方向上的发展包括深度生成重放[Shin等，2017]、DGDMN[Kamra等，2017]和双内存循环自组织[Parisi等，2018b].

#### 2.1.4 其他思路

​    其他一些人的相关工作包括Lean++[Polikar等，2001]、梯度偶发记忆（Gradient Episodic Memory）[LopezPaz等,2017]、Pathnet[Fernando等，2017]、记忆感知突触[Aljundi等，2017]、万物一体网络（One Big Net For Everything）[Schmidhuber,2018]、Phantom Sampling[Vebkatesan等, 2017]、主动长期网络（Active Long Term Memory Networks） [Furlanello等，2016]、概念器辅助的Backprop（Conceptor-Aided Backprop）[He and Jaeger, 2018]、门控网络（Gating Networks[Masse等,2018,Serra等, 2017]，PackNet[Mallya and Lazebnik,2017]，Diffusion-based Neuromodulation[Velez and Clune, 2017]，增量矩匹配（Incremental Moment Matching）[Lee等,2017b]，动态可扩展网络（Dynamically Expandable Networks）[Lee 等, 2017a]和增量正则化最小二乘（Incremental Regularized Least Squares）[Camoriano等, 2017]。

### 2.2 无监督学习   

 同样，也有一些*无监督学习*的工作研究。Goodrich和Areel[2014]研究了神经网络中的无监督在线聚类，以帮助减轻灾难性遗忘。它们提出了在前馈传递过程中，通过神经网络建立一条路径来选择神经元。每个神经元除了常规的权重外，还被分配了一个簇中心点。在新的任务中，当样本到达时，只选择簇中心点和样本接近的神经元。这可以看作是一种特殊的dropout training[Hinton等，2012]。Parisi等[2017]通过学习无监督的视觉表征来解决动作表征的LL。这种表征是基于发生频率域动作标签递增关联的。所提出的模型域用预先定义的动作类数量训练的模型相比，取得了具有竞争力的性能。

### 2.3 强化学习

​    在*强化学习*的应用中[Ring，1994]，除了上述的工作外（如Kirkpatrick等[2017]、Rusu等[2016]），Mankowirz等[2018]提出了一种名为Unicorn的持续学习代理架构。Unicorn代理被设计成具有同时学习包括新任务在内的多个任务的能力。该代理可以重用器积累的知识来有效的解决任务。最后，也是最重要的是，该架构旨在帮助代理解决具有深度依赖性的任务。其基本思想是离策略的（off-policy）学习多个任务，即当针对一个任务按照政策行事时，它可以利用这些经验来更新相关的任务。Kaplanis等[2018]从生物突触中获得灵感，并融入了不同时间尺度的可塑性。以减轻多时间尺度的灾难性遗忘。其突出巩固的思路与EWC的思路一直[Kirkpatrick等, 2017]。Lipton等[2016]提出了一种新的反馈塑造函数，可以学习当前产生灾难的可能性。它们称之为*内在恐惧（**intrinsic fear**）*，用于惩罚Q-learning目标。

### 2.4 学习评估框架

​    在灾难性遗忘的背景下，一些工作还提出了**评价框架**。Goodfellow等[2013a]评估了传统的方法，包括dropout training[Hinton等, 2012]和各种激活函数。Kemker等[2018]评估了更近的持续学习模型。Kemker等[2018]使用了大规模的数据集，并在LL环境下评估了模型在旧人物和新任务上的准确性。详见4.9节。在接下来的几节当中，我们将讨论一些有代表性的持续学习方法。